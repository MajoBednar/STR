Lexical Overlap:
    Train:    0.559
    Dev:      0.611
    Test:     0.638
--------------------------------------
Cosine Similarity:
    Train:    0.772 (LaBSE)
    Dev:      0.725 (LaBSE)
    Test:     0.797 (LaBSE)
Best Transformer: LaBSE
--------------------------------------
Siamese MLP:
    Train:    0.981     aug: 0.761
    Dev:      0.431     aug: 0.504
    Test:     0.537     aug: 0.610
Best hyperparameters:
    Transformer: LaBSE
    Architecture size: Medium+Big
    Activation function: ReLU
    Dropout: 0.4
    Optimizer: RMSprop
    Learning rate: 0.002
    Weight decay: 1e-6
    Early stopping: Loss
    Patience: 100
    Batch size: 32
    Epochs: 143
--------------------------------------
Siamese LSTM:
    Train:    0.951     aug: ?
    Dev:      0.700     aug: 0.703
    Test:     0.783     aug: 0.782
Best hyperparameters:
    Transformer: LaBSE
    Hidden dim factor: 3
    Number of layers: 1
    Optimizer: RMSprop
    Learning rate: 0.000544
    Weight decay: 0.0006
    Early stopping: Loss
    Patience: -
    Batch size: 32
    Epochs: 8
--------------------------------------
--------------------------------------
Siamese MLP augmented:
    Train:    0.
    Dev:      0.
    Test:     0.
Best hyperparameters:
    Transformer: LaBSE
    Architecture size: Medium+Big
    Activation function: ReLU
    Dropout: 0.4
    Optimizer: RMSprop
    Learning rate: 0.002
    Weight decay: 1e-6
    Early stopping: Loss
    Patience: 100
    Batch size: 32
    Epochs: 143
--------------------------------------
Siamese LSTM augmented:
    Train:    0.
    Dev:      0.
    Test:     0.
Best hyperparameters:
    Transformer: LaBSE
    Hidden dim factor: 3
    Number of layers: 1
    Optimizer: RMSprop
    Learning rate: 0.000544
    Weight decay: 0.0006
    Early stopping: Loss
    Patience: -
    Batch size: 32
    Epochs: 8