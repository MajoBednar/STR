Lexical Overlap:
    Train:    0.559
    Dev:      0.611
    Test:     0.638
--------------------------------------
Cosine Similarity:
    Train:    0.772 (LaBSE)
    Dev:      0.725 (LaBSE)
    Test:     0.797 (LaBSE)
Best Transformer: LaBSE
--------------------------------------
Siamese MLP:
    Train:    0.981
    Dev:      0.431
    Test:     0.537
Best hyperparameters:
    Transformer: LaBSE
    Architecture size: Medium+Big
    Activation function: ReLU
    Dropout: 0.4
    Optimizer: RMSprop
    Learning rate: 0.002
    Weight decay: 1e-6
    Early stopping: Loss
    Patience: 100
    Batch size: 32
    Epochs: 143
--------------------------------------
Siamese LSTM:
    Train:    0.951
    Dev:      0.700
    Test:     0.783
Best hyperparameters:
    Transformer: LaBSE
    Hidden dim factor: 3
    Number of layers: 1
    Optimizer: RMSprop
    Learning rate: 0.000544
    Weight decay: 0.0006
    Early stopping: Loss
    Patience: -
    Batch size: 32
    Epochs: 8
--------------------------------------