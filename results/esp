Lexical Overlap:
    Train:    0.618
    Dev:      0.634
    Test:     0.599
--------------------------------------
Cosine Similarity:
    Train:    0.651 (mBERT)
    Dev:      0.644 (LaBSE)
    Test:     0.599 (mBERT)
Best Transformer: mBERT, runner-up LaBSE
--------------------------------------
Siamese MLP:
    Train:    0.716     aug: 0.935
    Dev:      0.553     aug: 0.479
    Test:     0.345     aug: 0.434
Best hyperparameters:
    Transformer: XLMR
    Architecture size: Big+Big
    Activation function: ReLU
    Dropout: 0.2
    Optimizer: RMSprop
    Learning rate: 0.0004
    Weight decay: 3e-6
    Early stopping: Loss
    Patience: 30
    Batch size: 16
    Epochs: 29
--------------------------------------
Siamese LSTM:
    Train:    0.921     aug: 0.926
    Dev:      0.688     aug: 0.587
    Test:     0.608     aug: 0.526
Best hyperparameters:
    Transformer: mBERT
    Hidden dim factor: 1
    Number of layers: 1
    Optimizer: Adam
    Learning rate: 1.75e-5
    Weight decay: 0.00335
    Early stopping: Loss
    Patience: 20
    Batch size: 32
    Epochs: 56
--------------------------------------
--------------------------------------
Siamese MLP augmented:
    Train:    0.
    Dev:      0.
    Test:     0.
Best hyperparameters:
    Transformer: XLMR
    Architecture size: Big+Big
    Activation function: ReLU
    Dropout: 0.2
    Optimizer: RMSprop
    Learning rate: 0.0004
    Weight decay: 3e-6
    Early stopping: Loss
    Patience: 30
    Batch size: 16
    Epochs: 29
--------------------------------------
Siamese LSTM augmented:
    Train:    0.
    Dev:      0.
    Test:     0.
Best hyperparameters:
    Transformer: mBERT
    Hidden dim factor: 1
    Number of layers: 1
    Optimizer: Adam
    Learning rate: 1.75e-5
    Weight decay: 0.00335
    Early stopping: Loss
    Patience: 20
    Batch size: 32
    Epochs: 56