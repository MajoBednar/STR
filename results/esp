Lexical Overlap:
    Train:    0.618
    Dev:      0.634
    Test:     0.599
--------------------------------------
Cosine Similarity:
    Train:    0.651 (mBERT)
    Dev:      0.644 (LaBSE)
    Test:     0.599 (mBERT)
Best Transformer: mBERT, runner-up LaBSE
--------------------------------------
Siamese MLP:
    Train:    0.716
    Dev:      0.553
    Test:     0.345
Best hyperparameters:
    Transformer: XLMR
    Architecture size: Big+Big
    Activation function: ReLU
    Dropout: 0.2
    Optimizer: RMSprop
    Learning rate: 0.0004
    Weight decay: 3e-6
    Early stopping: Loss
    Patience: 30
    Batch size: 16
    Epochs: 29
--------------------------------------
Siamese LSTM:
    Train:    0.
    Dev:      0.
    Test:     0.
Best hyperparameters:
    Transformer:
    Hidden dim factor:
    Number of layers:
    Optimizer:
    Learning rate:
    Weight decay:
    Early stopping:
    Patience:
    Batch size:
    Epochs:
--------------------------------------