Lexical Overlap:
    Train:    0.618
    Dev:      0.634
    Test:     0.599
--------------------------------------
Cosine Similarity:
    Train:    0.651 (mBERT)
    Dev:      0.644 (LaBSE)
    Test:     0.599 (mBERT)
Best Transformer: mBERT, runner-up LaBSE
--------------------------------------
Siamese MLP:
    Train:    0.716
    Dev:      0.553
    Test:     0.345
Best hyperparameters:
    Transformer: XLMR
    Architecture size: Big+Big
    Activation function: ReLU
    Dropout: 0.2
    Optimizer: RMSprop
    Learning rate: 0.0004
    Weight decay: 3e-6
    Early stopping: Loss
    Patience: 30
    Batch size: 16
    Epochs: 29
--------------------------------------
Siamese LSTM:
    Train:    0.922
    Dev:      0.682
    Test:     0.587
Best hyperparameters:
    Transformer: mBERT
    Hidden dim factor: 1
    Number of layers: 1
    Optimizer: Adam
    Learning rate: 1.75e-5
    Weight decay: 0.00335
    Early stopping: Loss
    Patience: 20
    Batch size: 32
    Epochs: 56
--------------------------------------