Lexical Overlap:
    Train:    0.551
    Dev:      0.575
    Test:     0.511
--------------------------------------
Cosine Similarity:
    Train:    0.714 (LaBSE)
    Dev:      0.800 (LaBSE)
    Test:     0.696 (LaBSE)
Best Transformer: LaBSE
--------------------------------------
Siamese MLP:
    Train:    0.626     aug: -0.855
    Dev:      0.556     aug: -0.211
    Test:     0.554     aug: -0.155
Best hyperparameters:
    Transformer: miniLM
    Architecture size: Medium+Small
    Activation function: LeakyReLU
    Dropout: 0.165
    Optimizer: Adam
    Learning rate: 1.1e-5
    Weight decay: 0.0003
    Early stopping: Loss
    Patience: -
    Batch size: 64
    Epochs: 27
--------------------------------------
Siamese LSTM:
    Train:    0.995     aug: 0.931
    Dev:      0.776     aug: 0.731
    Test:     0.714     aug: 0.647
Best hyperparameters:
    Transformer: LaBSE
    Hidden dim factor: 3
    Number of layers: 1
    Optimizer: RMSprop
    Learning rate: 0.0001
    Weight decay: 0.0004
    Early stopping: Correlation
    Patience: -
    Batch size: 64
    Epochs: 14
--------------------------------------
--------------------------------------
Siamese MLP augmented:
    Train:    0.996
    Dev:      0.634
    Test:     0.484
Best hyperparameters:
    Transformer: miniLM
    Architecture size: Small+Medium
    Activation function: ReLU
    Dropout: 0.003
    Optimizer: Adam
    Learning rate: 0.00043
    Weight decay: 5.2e-6
    Early stopping: None
    Patience: -
    Batch size: 32
    Epochs: 108
--------------------------------------
Siamese LSTM augmented:
    Train:    0.928
    Dev:      0.758
    Test:     0.700
Best hyperparameters:
    Transformer: LaBSE
    Hidden dim factor: 3
    Number of layers: 1
    Optimizer: RMSprop
    Learning rate: 7.57e-5
    Weight decay: 1e-6
    Early stopping: Correlation
    Patience: -
    Batch size: 64
    Epochs: 2