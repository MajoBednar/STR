Lexical Overlap:
    Train:    0.551
    Dev:      0.575
    Test:     0.511
--------------------------------------
Cosine Similarity:
    Train:    0.714 (LaBSE)
    Dev:      0.800 (LaBSE)
    Test:     0.696 (LaBSE)
Best Transformer: LaBSE
--------------------------------------
Siamese MLP:
    Train:    0.626
    Dev:      0.556
    Test:     0.554
Best hyperparameters:
    Transformer: miniLM
    Architecture size: Medium+Small
    Activation function: LeakyReLU
    Dropout: 0.165
    Optimizer: Adam
    Learning rate: 1.1e-5
    Weight decay: 0.0003
    Early stopping: Loss
    Patience: -
    Batch size: 64
    Epochs: 27
--------------------------------------
Siamese LSTM:
    Train:    0.
    Dev:      0.
    Test:     0.
Best hyperparameters:
    Transformer:
    Hidden dim factor:
    Number of layers:
    Optimizer:
    Learning rate:
    Weight decay:
    Early stopping:
    Patience:
    Batch size:
    Epochs:
--------------------------------------