Lexical Overlap:
    Train:    0.551
    Dev:      0.575
    Test:     0.511
--------------------------------------
Cosine Similarity:
    Train:    0.714 (LaBSE)
    Dev:      0.800 (LaBSE)
    Test:     0.696 (LaBSE)
Best Transformer: LaBSE
--------------------------------------
Siamese MLP:
    Train:    0.626
    Dev:      0.556
    Test:     0.554
Best hyperparameters:
    Transformer: miniLM
    Architecture size: Medium+Small
    Activation function: LeakyReLU
    Dropout: 0.165
    Optimizer: Adam
    Learning rate: 1.1e-5
    Weight decay: 0.0003
    Early stopping: Loss
    Patience: -
    Batch size: 64
    Epochs: 27
--------------------------------------
Siamese LSTM:
    Train:    0.995
    Dev:      0.776
    Test:     0.714
Best hyperparameters:
    Transformer: LaBSE
    Hidden dim factor: 3
    Number of layers: 1
    Optimizer: RMSprop
    Learning rate: 0.0001
    Weight decay: 0.0004
    Early stopping: Correlation
    Patience: -
    Batch size: 64
    Epochs: 14
--------------------------------------