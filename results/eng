Lexical Overlap:
    Train:    0.541
    Dev:      0.535
    Test:     0.499
--------------------------------------
Cosine Similarity:
    Train:    0.795 (miniLM)
    Dev:      0.796 (miniLM)
    Test:     0.782 (miniLM)
Best Transformer: miniLM
--------------------------------------
Siamese MLP:
    Train:    0.997
    Dev:      0.690
    Test:     0.673
Best hyperparameters:
    Transformer: miniLM
    Architecture size: Medium+Small
    Activation function: ReLU
    Dropout: 0.001
    Optimizer: RMSprop
    Learning rate: 4.84e-5
    Weight decay: 3.6e-6
    Early stopping: None
    Patience: -
    Batch size: 16
    Epochs: 191
--------------------------------------
Siamese LSTM:
    Train:   >0.900
    Dev:      0.765
    Test:     0.748
Best hyperparameters:
    Transformer: LaBSE
    Hidden dim factor: 3
    Number of layers: 1
    Optimizer: RMSprop
    Learning rate: 2.365e-5
    Weight decay: 9.376e-5
    Early stopping: None
    Patience: -
    Batch size: 64
    Epochs: 15
--------------------------------------