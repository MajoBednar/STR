Lexical Overlap:
    Train:    0.695
    Dev:      0.707
    Test:     0.730
--------------------------------------
Cosine Similarity:
    Train:    0.765 (LaBSE)
    Dev:      0.811 (LaBSE)
    Test:     0.728 (LaBSE)
Best Transformer: LaBSE
--------------------------------------
Siamese MLP:
    Train:    0.992
    Dev:      0.592
    Test:     0.471
Best hyperparameters:
    Transformer: miniLM
    Architecture size: Medium+Medium
    Activation function: LeakyReLU
    Dropout: 3e-4
    Optimizer: Adam
    Learning rate: 1.5e-3
    Weight decay: 4.2e-6
    Early stopping: Loss
    Patience: 30
    Batch size: 16
    Epochs: 105
--------------------------------------
Siamese LSTM: (only 20 trials)
    Train:    0.991
    Dev:      0.626
    Test:     0.604
Best hyperparameters:
    Transformer: mBERT
    Hidden dim factor: 1/2
    Number of layers: 1
    Optimizer: RMSprop
    Learning rate: 0.0004
    Weight decay: 7e-6
    Early stopping: Correlation
    Patience: 20
    Batch size: 32
    Epochs: 20
--------------------------------------