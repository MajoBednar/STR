Lexical Overlap:
    Train:    0.695
    Dev:      0.707
    Test:     0.730
--------------------------------------
Cosine Similarity:
    Train:    0.765 (LaBSE)
    Dev:      0.811 (LaBSE)
    Test:     0.728 (LaBSE)
Best Transformer: LaBSE
--------------------------------------
Siamese MLP:
    Train:    0.992     aug: 0.995
    Dev:      0.592     aug: 0.448
    Test:     0.471     aug: 0.492
Best hyperparameters:
    Transformer: miniLM
    Architecture size: Medium+Medium
    Activation function: LeakyReLU
    Dropout: 3e-4
    Optimizer: Adam
    Learning rate: 1.5e-3
    Weight decay: 4.2e-6
    Early stopping: Loss
    Patience: 30
    Batch size: 16
    Epochs: 105
--------------------------------------
Siamese LSTM:
    Train:    0.958     aug: 0.926
    Dev:      0.809     aug: 0.801
    Test:     0.718     aug: 0.725
Best hyperparameters:
    Transformer: LaBSE
    Hidden dim factor: 3
    Number of layers: 1
    Optimizer: SGD
    Learning rate: 0.006
    Weight decay: 1.29e-5
    Early stopping: Correlation
    Patience: 20
    Batch size: 16
    Epochs: 100
--------------------------------------
--------------------------------------
Siamese MLP augmented:
    Train:    0.995
    Dev:      0.448
    Test:     0.492
Best hyperparameters:
    Transformer: miniLM
    Architecture size: Medium+Medium
    Activation function: LeakyReLU
    Dropout: 3e-4
    Optimizer: Adam
    Learning rate: 1.5e-3
    Weight decay: 4.2e-6
    Early stopping: Loss
    Patience: 30
    Batch size: 16
    Epochs: 105
--------------------------------------
Siamese LSTM augmented:
    Train:    0.926
    Dev:      0.801
    Test:     0.725
Best hyperparameters:
    Transformer: LaBSE
    Hidden dim factor: 3
    Number of layers: 1
    Optimizer: SGD
    Learning rate: 0.006
    Weight decay: 1.29e-5
    Early stopping: Correlation
    Patience: 20
    Batch size: 16
    Epochs: 100