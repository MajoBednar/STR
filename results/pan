Lexical Overlap:
    Train:   -0.242
    Dev:     -0.118
    Test:    -0.360
--------------------------------------
Cosine Similarity:
    Train:    0.018 (miniLM)
    Dev:      0.026 (LaBSE)
    Test:    -0.046 (miniLM)
Best Transformer: all roughly the same
--------------------------------------
Siamese MLP:
    Train:    0.900
    Dev:      0.688
    Test:     0.587
Best hyperparameters:
    Transformer: XLMR
    Architecture size: Small+Medium
    Activation function: ReLU
    Dropout: 0.4
    Optimizer: RMSprop
    Learning rate: 0.001
    Weight decay: 2.4e-6
    Early stopping: None
    Patience: -
    Batch size: 64
    Epochs: 124
--------------------------------------
Siamese LSTM:
    Train:    0.
    Dev:      0.
    Test:     0.
Best hyperparameters:
    Transformer:
    Hidden dim factor:
    Number of layers:
    Optimizer:
    Learning rate:
    Weight decay:
    Early stopping:
    Patience:
    Batch size:
    Epochs:
--------------------------------------