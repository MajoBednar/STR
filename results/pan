Lexical Overlap:
    Train:   -0.242
    Dev:     -0.118
    Test:    -0.360
--------------------------------------
Cosine Similarity:
    Train:    0.018 (miniLM)
    Dev:      0.026 (LaBSE)
    Test:    -0.046 (miniLM)
Best Transformer: all roughly the same
--------------------------------------
Siamese MLP:
    Train:    0.900     aug: 0.112
    Dev:      0.688     aug: 0.594
    Test:     0.587     aug: 0.519
Best hyperparameters:
    Transformer: XLMR
    Architecture size: Small+Medium
    Activation function: ReLU
    Dropout: 0.4
    Optimizer: RMSprop
    Learning rate: 0.001
    Weight decay: 2.4e-6
    Early stopping: None
    Patience: -
    Batch size: 64
    Epochs: 124
--------------------------------------
Siamese LSTM:
    Train:    0.937     aug: 0.206
    Dev:      0.594     aug: 0.549
    Test:     0.439     aug: 0.395
Best hyperparameters:
    Transformer: mBERT
    Hidden dim factor: 3
    Number of layers: 2
    Optimizer: Adam
    Learning rate: 3.3e-5
    Weight decay: 4.5e-5
    Early stopping: None
    Patience: -
    Batch size: 16
    Epochs: 3
--------------------------------------
--------------------------------------
Siamese MLP augmented:
    Train:    0.069
    Dev:      0.689
    Test:     0.523
Best hyperparameters:
    Transformer: mBERT
    Architecture size: Small+Medium
    Activation function: LeakyReLU
    Dropout: 0.445
    Optimizer: Adam
    Learning rate: 0.0021
    Weight decay: 6e-6
    Early stopping: None
    Patience: -
    Batch size: 64
    Epochs: 92
--------------------------------------
Siamese LSTM augmented:
    Train:    0.323
    Dev:      0.603
    Test:     0.493
Best hyperparameters:
    Transformer: LaBSE
    Hidden dim factor: 2
    Number of layers: 1
    Optimizer: RMSprop
    Learning rate: 1.23e-5
    Weight decay: 0.0027
    Early stopping: Correlation
    Patience: -
    Batch size: 16
    Epochs: 17